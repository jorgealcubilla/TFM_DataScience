library(dplyr)
setwd("./")
testData=read.csv("test_r",stringsAsFactors = FALSE)
str(testData)
head(testData)
summary(testData)
#testData$cam = as.factor(testData$cam)
testData$error_th10 = as.factor(testData$error_th10)
# Features relevant for k-means clustering:
clusterData = testData[c('error','ntrue','overlapping','bright','AvgDistance')]
# Variables normalization:
clusterDataScaled=scale(clusterData)
# Number of clusters (based on "Elbow method", see ANNEX below)
NUM_CLUSTERS=15
# k-means tends to converge to local minimums, being highly sensitive to initialization conditions.
set.seed(1234)
Model=kmeans(clusterDataScaled,NUM_CLUSTERS)
clusterData$Clusters=Model$cluster
clusterData$img =testData$img
clusterData$cam =testData$cam
# Let´s see clusters features:
table(clusterData$Clusters)
aggregate(clusterData[,-7:-8], by = list(clusterData$Clusters), median)
clusterData %>%
group_by(Clusters) %>%
summarise(maxError=max(error), minError=min(error), maxTrue = max(ntrue), minTrue=min(ntrue),
maxOvlap=max(overlapping), minOvlap = min(overlapping), maxBright=max(bright),
minBright = min(bright),maxDispers = max(AvgDistance), minDispers = min(AvgDistance))
# And let´s see the prediction error levels split by cluster:
table(clusterData$Clusters,testData[,13])
## Cluster# 14:
clusterData[clusterData$Clusters == "14",]
## Clusters# 6:
clusterData[clusterData$Clusters == "6",]
## Cluster# 9:
clusterData[clusterData$Clusters == "9",]
## Cluster# 15:
clusterData[clusterData$Clusters == "15",]
## Cluster# 1:
clusterData[clusterData$Clusters == "1",]
Intra <- (nrow(clusterData[,-6:-7])-1)*sum(apply(clusterData[,-7:-8],2,var))
for (i in 2:20) Intra[i] <- sum(kmeans(clusterData[,-7:-8], centers=i)$withinss)
library(dplyr)
setwd("./")
testData=read.csv("test_r",stringsAsFactors = FALSE)
str(testData)
head(testData)
summary(testData)
#testData$cam = as.factor(testData$cam)
testData$error_th10 = as.factor(testData$error_th10)
# Features relevant for k-means clustering:
clusterData = testData[c('error','ntrue','overlapping','bright','AvgDistance')]
# Variables normalization:
clusterDataScaled=scale(clusterData)
# Number of clusters (based on "Elbow method", see ANNEX below)
NUM_CLUSTERS=15
# k-means tends to converge to local minimums, being highly sensitive to initialization conditions.
set.seed(1234)
Model=kmeans(clusterDataScaled,NUM_CLUSTERS)
clusterData$Clusters=Model$cluster
clusterData$img =testData$img
clusterData$cam =testData$cam
# Let´s see clusters features:
table(clusterData$Clusters)
aggregate(clusterData[,-7:-8], by = list(clusterData$Clusters), median)
clusterData %>%
group_by(Clusters) %>%
summarise(maxError=max(error), minError=min(error), maxTrue = max(ntrue), minTrue=min(ntrue),
maxOvlap=max(overlapping), minOvlap = min(overlapping), maxBright=max(bright),
minBright = min(bright),maxDispers = max(AvgDistance), minDispers = min(AvgDistance))
# And let´s see the prediction error levels split by cluster:
table(clusterData$Clusters,testData[,13])
## Cluster# 14:
clusterData[clusterData$Clusters == "14",]
## Clusters# 6:
clusterData[clusterData$Clusters == "6",]
## Cluster# 9:
clusterData[clusterData$Clusters == "9",]
## Cluster# 15:
clusterData[clusterData$Clusters == "15",]
## Cluster# 1:
clusterData[clusterData$Clusters == "1",]
Intra <- (nrow(clusterData[,-6:-7])-1)*sum(apply(clusterData[,-7:-8],2,var))
for (i in 2:20) Intra[i] <- sum(kmeans(clusterData[,-7:-8], centers=i)$withinss)
plot(1:20, Intra, type="b", xlab="Number of Clusters", ylab="Mean distance to centroids")
## -------------------------------------------------------------------------
##### Saving clusters data #####
write.csv(clusterData[, c('img','Clusters')], 'clustersR.csv')
## -------------------------------------------------------------------------
## -------------------------------------------------------------------------
