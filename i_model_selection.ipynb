{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection and analysis of a deep learning model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1475,
     "status": "ok",
     "timestamp": 1540355661803,
     "user": {
      "displayName": "Jorge Alc",
      "photoUrl": "",
      "userId": "02955786494308206669"
     },
     "user_tz": -120
    },
    "id": "Cb0sC2wwKK7A",
    "outputId": "7fd48a91-c285-4a05-f67f-41bbb9f588f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JORGE\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Libraries required for this section:\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from keras import models\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) Selection:\n",
    "\n",
    "The problem of counting number of object instances visible in a single image has been studied in depth by a wide variety of algorithm-based methodologies which can be classified, according to ['Crowd counting and proﬁling: Methodology and evaluation[1]'](#ref), in three groups: counting by clustering, counting by detection and counting by regresion. \n",
    "\n",
    "We have selected a 'counting by regresion' model because they have demonstrated to be the most accuareate and fastest ones, setting the state-of-the-art results in most of the benchmarks so far.\n",
    "\n",
    "The selected model is based on the ['Towards perspective-free object counting with deep learning'](http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf) paper by Daniel Oñoro-Rubio and Roberto J. López-Sastre [[2]](#ref).\n",
    "\n",
    "This paper was chosen because:\n",
    "- It is supported by extensive and detailed documentation such as a GitHub project that includes trained models,\n",
    "- The models subject to analysis in this paper are simpler, more compact and less computational expensive than other similar ones such as ['the Crowd_CNN model by Zhang et al. [3]'](#ref).\n",
    "- A priori, one of the data sets used by this paper for experimental purpuses, TRANCOS, could have an interesting practical application to traffic density estimation. \n",
    "- It claims that their models outperform 'state-of-the-art' results as of 2016.\n",
    "\n",
    "This paper develops 2 deep learning models based on CNN, Counting_CNN and Hydra_CNN, for objects density map estimation.<br>\n",
    "\n",
    "I will use the '3-head' Hydra_CNN architecture since, in accordance with the paper, it is the model that provides the best results in terms of metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) Analysis:\n",
    "\"3-head Hydra model\" (based on a 3-head Hydra_CNN**) phases:<br>\n",
    "1. Data preprocessing: original image breakdown into 115x115 overlapped patches \n",
    "2. 3-head Hydra_CNN processing: Obtaining densitiy map estimation for each 115x115 patch **(MODEL TRANSLATION TO KERAS inclusive)**\n",
    "3. Density map assembly: Assembling all the estimated maps to get the density map for the whole original image.\n",
    "\n",
    "**Although the model includes a 3-head Hydra_CNN, the CNN is just a part (the key part) of the model.\n",
    "That´s why we will refer to the whole model as '3-head Hydra model' (excluding the word 'CNN' from the model´s name).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Preprocessing:\n",
    "\n",
    "Steps:<br>\n",
    "1.1) Extract from the original image all the consecutive 115x115 patches with a stride of 10 pixels. <br>\n",
    "1.2) Build a 'pyramid' of 3 different scale levels ('s') from each extracted patch: <br>\n",
    "         - s_0: The original patch.\n",
    "         - s_1: Centered crop of 66% of the original patch.\n",
    "         - s_2: Centered crop of 33% of the original patch.\n",
    "1.3) Resize s_0, s_1 and s_2 to 72x72 pixels to feed the Hydra_CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following functions will be used in this phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    To be used in 1.1)\n",
    "    Generates a cartesian product of input arrays.\n",
    "    This function will provide a list of coordinates (positions) required by 'get_dense_pos' function (see below)\n",
    "    to select those that will be used to build the patches (115x115).\n",
    "    \n",
    "    \"\"\"    \n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    m = int(n / arrays[0].size)\n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m,1:])\n",
    "        for j in range(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m,1:] = out[0:m,1:]\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_pos(heith, width, pw, stride = 1):\n",
    "    \n",
    "    '''\n",
    "    To be used in 1.1)\n",
    "    \n",
    "    The patches will be created by selecting points of the image as the center of the patch.\n",
    "    Thus, the points of the image that generate patches whose coordinates go beyond the image´s perimeter \n",
    "    will not be selected.\n",
    "    \n",
    "    This function provides a dense list of points that will be used to build the patches (115x115).\n",
    "    \n",
    "    Those points will be selected from the area of the image whose points are at a distance from image´s perimeter \n",
    "    equal or higher than half of the patch height (or width), with a stride set by the 'stride' parameter \n",
    "    (stride=10 in Hydra model).\n",
    "    \n",
    "    @param heith: image height.\n",
    "    @param width: image width.\n",
    "    @param pw: patch with.\n",
    "    @param stride: stride.\n",
    "  \n",
    "    '''    \n",
    "    # Computes half a patch (height or width, it´s the same)\n",
    "    dx=dy=int(pw/2)\n",
    "    \n",
    "    # Coordinates of the area of the image whose points are at a distance from its perimeter equal or higher\n",
    "    #than half of a patch: \n",
    "    pos = cartesian( (range(dx, heith - dx, stride), range(dy, width -dy, stride) ) )\n",
    "    bot_line = cartesian( (heith - dx -1, range(dy, width -dy, stride) ) )\n",
    "    right_line = cartesian( (range(dx, heith - dx, stride), width -dy - 1) )\n",
    "    \n",
    "    return np.vstack( (pos, bot_line, right_line) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEscales(lim, n_scales, input_pw):\n",
    "    '''  \n",
    "    To be used in 1.2) and 1.3)\n",
    "    \n",
    "    Builds a 'pyramid' of different scale levels for each extracted patch, and\n",
    "    resizes the crops (s_0, s_1 and s_2 in our case) to the input size required to feed the Hydra_CNN.\n",
    "\n",
    "    @param lim: list of patches related to the original image\n",
    "    @param n_scales: number of different scale levels (3 in our case)\n",
    "    @param input_pw: input size (72x72 pixels)\n",
    "    \n",
    "    '''\n",
    "    out_list = [] #List of 'pyramids' corresponding to all the patches extracted from the original image\n",
    "    for im in lim:\n",
    "        ph, pw = im.shape[0:2] # it will get the patch width and height (115x115)\n",
    "        scaled_im_list = [] #list of crops ('pyramid') related to a specific patch\n",
    "        \n",
    "        #Crops generating and resizing:\n",
    "        for s in range(n_scales):\n",
    "            ch = int(s * ph / (2*n_scales)) \n",
    "            cw = int(s * pw / (2*n_scales))\n",
    "            \n",
    "            crop_im = im[ch:ph-ch, cw:pw - cw]\n",
    "            \n",
    "            #s=0, s_0 = original patch = (115x115)\n",
    "            #s=1, s_1 = 66% of original patch = (77x77)\n",
    "            #s=2, s_2 = 33% of original patch = (39x39)\n",
    "            \n",
    "            #Resizes the crops (s_0, s_1 and s_2 in our case) to the input size required to feed the Hydra_CNN (72x72)\n",
    "            scaled_im_list.append(resize(crop_im, (input_pw, input_pw)))\n",
    "        \n",
    "        out_list.append(scaled_im_list)\n",
    "        \n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 3-head Hydra_CNN processing: Obtaining densitiy map estimation for each 115x115 patch \n",
    "\n",
    "Hydra_CNN works learning a multiscale non-linear regressor from the input image features to obtain an object density map.<br>\n",
    "Therefore, each preprocessed **115x115 patch** extracted from the original image **('pyramid'** made up of **s_0, s_1 and s_2 crops resized to 72x72)** goes through the Hydra_CNN to obtain their **density map**.\n",
    "\n",
    "### 2.1. \"3-head Hydra_CNN\" Architecture:\n",
    "\n",
    "**- Input:**<br>\n",
    "The network is fed with 'pyramids' made up of **3 input crops (s_0, s_1 and s_2)**, extracted from each 115x115 patch and rescaled to a size of 72x72.\n",
    "\n",
    "**- Heads:**<br>\n",
    "Each head will learn the paterns for a particular scale level from the input 'pyramid'.<br>\n",
    "Therefore, **there are 3 heads**.<br>\n",
    "\n",
    "All the heads have the same architecture, consisting of **5 convolutional layers**:<br>\n",
    "**Conv1** and **Conv2** layers have filters of size 7x7 with a depth of 32, and they are followed by a max-pooling layer, with 2x2 kernel size.<br>\n",
    "The **Conv3** layer has 5x5 filters with a depth of 64, followed by a max-pooling layer, with 2x2 kernel size.<br>\n",
    "**Conv4** y **Conv5** layers are made up of 1x1 filters with a depth of 1000 and 400, respectively.<br>\n",
    "All these convolutional layers are followed by rectified linear units (ReLU).\n",
    "\n",
    "![title](./Notebook_pics/CNN_layers.png)\n",
    "**Fig.1.**  *Heads structure: consisting of 5 covolutional layers (figure extracted from the paper ['Towards perspective-free object counting with deep learning'](http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf))*.\n",
    "\n",
    "**- Body**:<br>\n",
    "The output of the heads is a set of features describing the images at different scales.<br> \n",
    "These features are concatedated to feed the 'body', which is made up of fully-connected layers.<br>\n",
    "There are 3 layers:<br>\n",
    "\n",
    "**Fc6** and **Fc7** have 512 neurons followed by a ReLU and a dropout layer, each one.<br>\n",
    "**Fc8** is the final layer, with 324 neurons, whose output is the object density map.<br>\n",
    "\n",
    "**Note: The format of the density map returned by this model is a vector with a dimension of 324.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frjirPT-KK7G"
   },
   "source": [
    "![title](./Notebook_pics/hydra_architecture.png)\n",
    "**Fig.2**. *n-head Hydra_CNN Architecture (figure extracted from the paper ['Towards perspective-free object counting with deep learning'](http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf))*.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Translation to Keras:\n",
    "The original model uses the Caffe library so, **an important contribution of this project** is the translation of the whole model from Caffe into Keras.\n",
    "\n",
    "Note: In this section, we will not include any specific element for training purposes. \n",
    "\n",
    "The translation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpyOfnXFKK7J"
   },
   "outputs": [],
   "source": [
    "def hydra_CNN():\n",
    "\n",
    "### CCNN_s0:\n",
    "    #Input:\n",
    "    head0_input = layers.Input(shape=(3,72,72))\n",
    "    #Conv1:\n",
    "    head0 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head0_input)\n",
    "    head0 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head0_conv1\", data_format=\"channels_first\")(head0)\n",
    "    head0 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head0)\n",
    "    #Conv2:\n",
    "    head0 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head0)\n",
    "    head0 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head0_conv2\", data_format=\"channels_first\")(head0)\n",
    "    head0 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head0)\n",
    "    #Conv3:\n",
    "    head0 = layers.ZeroPadding2D(padding=(2, 2), data_format=\"channels_first\")(head0)\n",
    "    head0 = layers.Conv2D(64, kernel_size=5, activation='relu', name=\"head0_conv3\", data_format=\"channels_first\")(head0)\n",
    "    #Conv4:\n",
    "    head0 = layers.Conv2D(1000, kernel_size=1, activation='relu', name=\"head0_conv4\", data_format=\"channels_first\")(head0)\n",
    "    #Conv5:\n",
    "    head0 = layers.Conv2D(400, kernel_size=1, activation='relu', name=\"head0_conv5\", data_format=\"channels_first\")(head0)\n",
    "\n",
    "\n",
    "### CCNN_s1:\n",
    "    #Input:\n",
    "    head1_input = layers.Input(shape=(3,72,72))\n",
    "    #Conv1:\n",
    "    head1 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head1_input)\n",
    "    head1 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head1_conv1\", data_format=\"channels_first\")(head1)\n",
    "    head1 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head1)\n",
    "    #Conv2:\n",
    "    head1 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head1)\n",
    "    head1 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head1_conv2\", data_format=\"channels_first\")(head1)\n",
    "    head1 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head1)\n",
    "    #Conv3:\n",
    "    head1 = layers.ZeroPadding2D(padding=(2, 2), data_format=\"channels_first\")(head1)\n",
    "    head1 = layers.Conv2D(64, kernel_size=5, activation='relu', name=\"head1_conv3\", data_format=\"channels_first\")(head1)\n",
    "    #Conv4:\n",
    "    head1 = layers.Conv2D(1000, kernel_size=1, activation='relu', name=\"head1_conv4\", data_format=\"channels_first\")(head1)\n",
    "    #Conv5:\n",
    "    head1 = layers.Conv2D(400, kernel_size=1, activation='relu', name=\"head1_conv5\", data_format=\"channels_first\")(head1)\n",
    "\n",
    "\n",
    "### CCNN_s2:\n",
    "    #Input:\n",
    "    head2_input = layers.Input(shape=(3,72,72))\n",
    "    #Conv1:\n",
    "    head2 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head2_input)\n",
    "    head2 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head2_conv1\", data_format=\"channels_first\")(head2)\n",
    "    head2 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head2)\n",
    "    #Conv2:\n",
    "    head2 = layers.ZeroPadding2D(padding=(3, 3), data_format=\"channels_first\")(head2)\n",
    "    head2 = layers.Conv2D(32, kernel_size=7, activation='relu', name=\"head2_conv2\", data_format=\"channels_first\")(head2)\n",
    "    head2 = layers.MaxPooling2D(pool_size=2, strides=2, data_format=\"channels_first\")(head2)\n",
    "    #Conv3:\n",
    "    head2 = layers.ZeroPadding2D(padding=(2, 2), data_format=\"channels_first\")(head2)\n",
    "    head2 = layers.Conv2D(64, kernel_size=5, activation='relu', name=\"head2_conv3\", data_format=\"channels_first\")(head2)\n",
    "    #Conv4:\n",
    "    head2 = layers.Conv2D(1000, kernel_size=1, activation='relu', name=\"head2_conv4\", data_format=\"channels_first\")(head2)\n",
    "    #Conv5:\n",
    "    head2 = layers.Conv2D(400, kernel_size=1, activation='relu', name=\"head2_conv5\", data_format=\"channels_first\")(head2)\n",
    "\n",
    "#Note: Caffe uses a \"channel_first\" format for inputs.\n",
    "\n",
    "## Concatenation of outputs from the CCNN layers (s0, s1 and s2):\n",
    "    body = layers.concatenate([head0, head1, head2], axis=1)\n",
    "    body = layers.Flatten()(body)\n",
    "#Note: A 'Flatten' layer is added, which it´s not mentioned in the paper, \n",
    "#to be able to feed the fully-connected layers properly. \n",
    "\n",
    "\n",
    "## Fully-connected layers:\n",
    "    #Fc6:\n",
    "    body = layers.Dense(512, activation='relu', name=\"body_fc6\")(body)\n",
    "    #Fc7:\n",
    "    body = layers.Dense(512, activation='relu', name=\"body_fc7\")(body)\n",
    "    #Fc8:\n",
    "    body = layers.Dense(324, activation='relu', name=\"body_fc8\")(body)\n",
    "\n",
    "\n",
    "## Model definition:\n",
    "    hydra_CNN_model = models.Model(inputs=[head0_input, head1_input, head2_input], outputs=body)\n",
    "\n",
    "    return hydra_CNN_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s have a look at an instance of a '3-head Hydra_CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1486
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1540355665508,
     "user": {
      "displayName": "Jorge Alc",
      "photoUrl": "",
      "userId": "02955786494308206669"
     },
     "user_tz": -120
    },
    "id": "qE7AjHdSKK7M",
    "outputId": "cab69d39-2757-4300-e5ee-824fc7045273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 72, 72)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3, 72, 72)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3, 72, 72)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 3, 78, 78)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 3, 78, 78)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 3, 78, 78)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "head0_conv1 (Conv2D)            (None, 32, 72, 72)   4736        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head1_conv1 (Conv2D)            (None, 32, 72, 72)   4736        zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head2_conv1 (Conv2D)            (None, 32, 72, 72)   4736        zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 36, 36)   0           head0_conv1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 36, 36)   0           head1_conv1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 36, 36)   0           head2_conv1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 32, 42, 42)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 32, 42, 42)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 32, 42, 42)   0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "head0_conv2 (Conv2D)            (None, 32, 36, 36)   50208       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head1_conv2 (Conv2D)            (None, 32, 36, 36)   50208       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head2_conv2 (Conv2D)            (None, 32, 36, 36)   50208       zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 18, 18)   0           head0_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 18, 18)   0           head1_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 18, 18)   0           head2_conv2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 32, 22, 22)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 32, 22, 22)   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 32, 22, 22)   0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "head0_conv3 (Conv2D)            (None, 64, 18, 18)   51264       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head1_conv3 (Conv2D)            (None, 64, 18, 18)   51264       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head2_conv3 (Conv2D)            (None, 64, 18, 18)   51264       zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "head0_conv4 (Conv2D)            (None, 1000, 18, 18) 65000       head0_conv3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "head1_conv4 (Conv2D)            (None, 1000, 18, 18) 65000       head1_conv3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "head2_conv4 (Conv2D)            (None, 1000, 18, 18) 65000       head2_conv3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "head0_conv5 (Conv2D)            (None, 400, 18, 18)  400400      head0_conv4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "head1_conv5 (Conv2D)            (None, 400, 18, 18)  400400      head1_conv4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "head2_conv5 (Conv2D)            (None, 400, 18, 18)  400400      head2_conv4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1200, 18, 18) 0           head0_conv5[0][0]                \n",
      "                                                                 head1_conv5[0][0]                \n",
      "                                                                 head2_conv5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 388800)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "body_fc6 (Dense)                (None, 512)          199066112   flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "body_fc7 (Dense)                (None, 512)          262656      body_fc6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "body_fc8 (Dense)                (None, 324)          166212      body_fc7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 201,209,804\n",
      "Trainable params: 201,209,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hydra_sample = hydra_CNN()\n",
    "hydra_sample.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Density map assembly: Assembling all the estimated maps to get the density map for the whole original image.\n",
    "\n",
    "Steps:<br>\n",
    "\n",
    "### 3.1) Reshape&resize Hydra_CNN´s outputs to get the density map for each patch:\n",
    "As mentioned before, the **output** of the Hydra_CNN is a **1x324 vector**.<br>\n",
    "In order to get the density map of each patch, their corresponding vector will be **reshaped to a 2-dimention array**.<br>\n",
    "Due to the max-pooling layers, the size of **density map estimations is 1/4 of the input image patch (18x18 pixels)**.<br>\n",
    "Thus, the reshaped output will have to be **resized** to the original size of the patch (115x115).<br>\n",
    "\n",
    "**The following function will be used for this purpose:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeDensityPatch(patch, opt_size):\n",
    "    '''\n",
    "    Takes a density map and resizes it to the opt_size.\n",
    "    \n",
    "    @param patch: input density map.\n",
    "    @param opt_size: output size (the original size of the patch, 115x115, in our case).\n",
    "        \n",
    "    The rescaling process will generate a density map whose associated count (pixel values addition) will not\n",
    "    necessarily match the count of the input density map.\n",
    "    Therefore, the input density map will be normalized before rescaling, then reversed after rescaling\n",
    "    and the final count adjusted.\n",
    "    \n",
    "    '''\n",
    "       \n",
    "    # Input normalization to values between 0 and 1:\n",
    "    patch_sum = patch.sum()   \n",
    "    p_max = patch.max()\n",
    "    p_min = patch.min()    \n",
    "    if patch_sum !=0: # Avoid 0 division\n",
    "        patch = (patch - p_min)/(p_max - p_min)\n",
    "    \n",
    "    # Resizing to the original size of the patch:\n",
    "    patch = resize(patch, opt_size)\n",
    "    \n",
    "    # Normalization reversal:\n",
    "    patch = patch*(p_max - p_min) + p_min\n",
    "    \n",
    "    # Count adjustment:\n",
    "    res_sum = patch.sum()\n",
    "    if res_sum != 0:\n",
    "        return patch * (patch_sum/res_sum)\n",
    "\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Assembly of all density map estimations:\n",
    "**All** the **density map estimations** for the extracted patchs are **aggregated on a single density** map with the size\n",
    "of the original image.<br>\n",
    "Since **patches overlap**, the times each coordinate of the final density map is estimated (votes) will be calculated and used to get the **average estimation** for each coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final function required to run the model is as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hydra model processing:\n",
    "\n",
    "def process(model, im, n_scales, base_pw, input_pw):\n",
    "    \n",
    "    '''\n",
    "    Parameters (values based on the paper´s methodology):\n",
    "    @param model: model to be used for density map estimation purposes (3-head Hydra_CNN)\n",
    "    @param im: image from which a density map will be estimated\n",
    "    @param n_scales: number of different scale levels to feed the Hydra_CNN as inputs (for the 3-head Hydra_CNN, \n",
    "                there will be 3: s:0, s_1, s_2)\n",
    "    @param base_pw: original size of patches to be extracted from the image (115x115 pixels)\n",
    "    @param inpu_pw: inputs size (72x72 pixels)\n",
    "    \n",
    "    '''\n",
    "#1) Data preprocessing: original image breakdown into 115x115 overlapped patches\n",
    "    #Steps:\n",
    "    \n",
    "    #1.1) Extract from the original image all the consecutive 115x115 patches with a stride of 10 pixels: \n",
    "            \n",
    "    # Obtaining a dense list of points (coordinates) from the image that will be used to build the patches (115x115)\n",
    "    [heith, width] = im.shape[0:2]\n",
    "    pos = get_dense_pos(heith, width, base_pw, stride=10) #Stride=10\n",
    "\n",
    "    # Initialize density matrix and votes counting\n",
    "    dens_map = np.zeros( (heith, width), dtype = np.float32 )  \n",
    "    count_map = np.zeros( (heith, width), dtype = np.int32 )  \n",
    "                \n",
    "    for ix, p in enumerate(pos): # Iterate for all patches            \n",
    "        dx=dy=int(base_pw/2) # Compute displacement from centers\n",
    "        x,y=p\n",
    "        sx=slice(x-dx, x+dx+1, None)\n",
    "        sy=slice(y-dy, y+dy+1, None)\n",
    "        crop_im=im[sx,sy,...]\n",
    "        h, w = crop_im.shape[0:2]\n",
    "        if h!=w or (h<=0):\n",
    "            continue\n",
    "    #1.2) Build a 'pyramid' of 3 different scale levels ('s') for each extracted patch, and,\n",
    "    #1.3) Resize s_0, s_1 and s_2 to 72x72 pixels to feed the Hydra_CNN.\n",
    "           \n",
    "        im_scales = extractEscales([crop_im], n_scales, input_pw)\n",
    "                        \n",
    "        head0_input = np.expand_dims(im_scales[0][0].copy().transpose(2,0,1), axis=0)\n",
    "        head1_input = np.expand_dims(im_scales[0][1].copy().transpose(2,0,1), axis=0)\n",
    "        head2_input = np.expand_dims(im_scales[0][2].copy().transpose(2,0,1), axis=0)\n",
    "            \n",
    "#2) 3-head Hydra_CNN processing: Obtaining densitiy map estimation for each 115x115 patch         \n",
    "        pred = model.predict([head0_input,  head1_input,  head2_input])\n",
    "\n",
    "#3) Density map assembly: Assembling all the estimated maps to get the density map for the whole original image\n",
    "\n",
    "    #3.1) Reshape&resize Hydra_CNN´s outputs to get the density map for each patch:\n",
    "        #Rashape to a 2-dimention array\n",
    "        p_side = int(np.sqrt( len( pred.flatten() ) )) \n",
    "        pred = pred.reshape(  (p_side, p_side) )\n",
    "            \n",
    "            # Resize it back to the original patch size (115x115 pixels)\n",
    "        pred = resizeDensityPatch(pred, crop_im.shape[0:2])          \n",
    "        pred[pred<0] = 0\n",
    "\n",
    "    #3.2) Assembly of all patches density map estimations:\n",
    "        # The estimated density map for each patch is added to the total density map:\n",
    "        dens_map[sx,sy] += pred\n",
    "        #Since overlapping occurs, a matrix is summing up the times (votes) each coordinate has been estimated, \n",
    "        #for afterwards average calculation purposes:\n",
    "        count_map[sx,sy] += 1\n",
    "\n",
    "    # Remove Zeros\n",
    "    count_map[ count_map == 0 ] = 1\n",
    "\n",
    "    # Final average density map for the whole original image:\n",
    "    dens_map = dens_map / count_map        \n",
    "        \n",
    "    return dens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "### References:\n",
    "[1]: Loy, C., Chen, K., Gong, S., Xiang, T.: Crowd counting and proﬁling: Methodology and evaluation. In: Modeling, Simulation and Visual Analysis of Crowds. (2013) <br>\n",
    "\n",
    "[2]: Oñoro-Rubio D., López-Sastre R.J. (2016) Towards Perspective-Free Object Counting with Deep Learning. In: Leibe B., Matas J., Sebe N., Welling M. (eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture Notes in Computer Science, vol 9911. Springer, Cham <br>\n",
    "\n",
    "[3]: Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep convolutional neural networks. In: CVPR. (June 2015) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hydra_model_v3 -flattenbody.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
